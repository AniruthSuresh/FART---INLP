# FART-INLP  
Repository for maintaining all code, experiments, and documentation related to the INLP Project.

---

## Mid-Submission Update

As of the mid-submission checkpoint, the repository contains **6 active branches**:

1. **`main`**  
   Hosts all essential documentation, reports, and submission-related materials.

2. **`FNET-pretrained-SWAG`**  
   Contains code and results comparing baseline BERT with its Fourier-enhanced counterpart (FNet), evaluated on the SWAG dataset.

3. **`BART-SST`**  
   Includes scripts for fine-tuning both base BART and Fourier-enhanced BART models on the SST dataset.

4. **`fftNET`**  
   A modified fork of [jacobfa/fft](https://github.com/jacobfa/fft), adapted for benchmarking on the CIFAR-10 dataset using Transformer, FFT, and FFTNet models.

5. **`FourierTransform`**  
   An unofficial fork of [lumia-group/fouriertransformer](https://github.com/lumia-group/fouriertransformer).  
   _Note: Minimal activity on this branch as of mid-submission._

6. **`jedi-base`**  
   An improved version of the [JEDI](https://github.com/Willy-Chan/JEDI) repository, with added metrics and support for fine-tuning BART-based models.

---

## Status

This repository is currently a work in progress. Active development is ongoing.
